---
title: "Lab 9 - HPC"
output: 
  # pdf_document: default
  # html_document: default
  tufte::tufte_html:
    css: style.css
link-citations: yes
---

# Learning goals

In this lab, you are expected to learn/put in practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=FALSE, echo=FALSE}
# install any missing packages
install.packages("microbenchmark")
```

## Problem 1: Think

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

_Answer here._

-   Tree-based models. When we try to build tree-based machine learning models like random forests, we can paralleized the process of building individual predictors/trees to save large amount of runtime. Since these models utilize bagging to reduce the variance of predictions, we usually build many individual trees which could be paralleized to save time. Some sample R packages that can help us include Rborist, h2o, and randomForestSRC.

-   Bootstrapping and cross-validation. During the model building and evaluation process, we often use bootstrapping to quantify the uncertainty of the model performance and cross-validation when we have little training data in settings like healthcare. These procedures both require calculating some metrics on different set of samples repeately, which can be efficiently paralleized with multiple processes. Some sample R packages that can help us include caret.

-   Matrix multiplications and GPU computations. One of main drive for modern machine learning is the use of large scale neural networks that can be trained efficiently using GPUs. When the model is particularly large, multiple GPUs are needed to run inference and training for the internal matrix operations. Some sample R packages that can help us include OpenCL and tensorflow.

https://cran.r-project.org/web/views/HighPerformanceComputing.html (3 examples and include package)

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x <- matrix(rpois(n*k, lambda), ncol=k)
  
  return(x)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(100),
  fun1alt(100)
)
```

How much faster?

_Answer here._

-   The mean runtime of 100 evaluations of the original function takes 227.4 seconds, and our version is only 29.8 seconds. This is almost 10 times faster.

2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  x[cbind(max.col(t(x)), 1:ncol(x))]
}

# Benchmarking
bench <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)

bench
```

_Answer here with a plot._

plot the two performance, distribution (boxplot) of the 100 runs for each method

```{r}
library(ggplot2)
bench_df <- as.data.frame(bench)

ggplot(bench_df, aes(x = expr, y = time)) +
  geom_boxplot() +
  labs(title = "Runtime Distribution of Two Algorithms",
       x = "Algorithm",
       y = "Runtime")

```



## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1
  cl <- makePSOCKcluster(ncpus)
  # STEP 2
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment())
  
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  # ans <- lapply(seq_len(R), function(i) {
  #   stat(dat[idx[,i], , drop=FALSE])
  # })
  
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  stopCluster(cl)
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example
for you to try:

```{r p3-test-boot}
library(parallel)
# Bootstrap of a linear regression model
my_stat <- function(d) {coef(lm(y~x, data=d))}

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x,y), my_stat, R=R, ncpus = 4)

ans0
t(apply(ans1, 2, quantile, probs = c(.025, .975)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3}
detectCores()

system.time(my_boot(dat = data.frame(x,y), my_stat, R=R, ncpus = 1))
system.time(my_boot(dat = data.frame(x,y), my_stat, R=R, ncpus = 4))
```

_Answer here._

-   Using 4 cores, we reduced the the clock time almost by half: 1.737 seconds v.s. 3.590 seconds

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


