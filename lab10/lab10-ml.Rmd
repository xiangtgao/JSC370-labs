---
title: "Lab 10 - Trees, Bagging, RF, Boosting XGBoost"
output: html_document
---

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/JSC370-2024/blob/main/data/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r eval=FALSE, include=FALSE}
install.packages(c("rpart", "rpart.plot", "randomForest", "gbm", "xgboost"))
```

### Load packages and data
```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)

heart <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/heart.csv") |>
  mutate(
    AHD = 1 * (AHD == "Yes"),
    ChestPain = factor(ChestPain),
    Thal = factor(Thal)
  )

head(heart)
```


---


## Question 1: Trees
- Split the `heart` data into training and testing (70-30%)

```{r}
set.seed(100)
train<-sample(1:nrow(heart), round(0.7*nrow(heart)))
heart_train<-heart[train,]
heart_test<-heart[-train,]
```

- Fit a classification tree using rpart, plot the full tree. We are trying to predict AHD. Set minsplit = 10, minbucket = 3, and do 10 cross validations.
```{r}
heart_tree <- rpart(
  AHD~., 
  method = 'class',
  control = list(cp=0,
                 minbucket=3,
                 minsplit=10,
                 xval=10), 
  data = heart_train
)
rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and find the optimal cp
```{r}
plotcp(heart_tree)
printcp(heart_tree)

optimalcp = heart_tree$cptable[which.min(heart_tree$cptable[, 'xerror']), "CP"]
```

- Prune the tree

```{r}
heart_tree_prune<-prune(heart_tree, cp=optimalcp)
rpart.plot(heart_tree_prune)
```

- Compute the test misclassification error

```{r}
heart_pred<-predict(heart_tree_prune, heart_test)
heart_pred <- as.data.frame(heart_pred)
heart_pred$AHD <- ifelse(heart_pred$'1'>0.5, 1, 0)

confmatrix_table <- table(true=heart_test$AHD, predicted=heart_pred$AHD)
confmatrix_table
mis_error = (11+16) / (33+16+11+31)
mis_error
mis_error = (confmatrix_table[1, 2]+confmatrix_table[2, 1]) / 
  (confmatrix_table[1, 2]+confmatrix_table[2, 1]+confmatrix_table[1, 1]+confmatrix_table[2, 2])
mis_error
```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)

```{r}
heart_tree<-rpart(
  AHD~.,
  data = heart,
  method = 'class',
  control = list(cp=optimalcp)
)
```

- Find the Out of Bag (OOB) error for tree

```{r}
heart_tree$cptable
min(heart_tree$cptable[, 'xerror'])* nrow(heart)
```


---

## Question 2: Bagging, Random Forest

- Compare the performance of classification trees (above), bagging, random forests for predicting heart disease based on the ``heart`` data.

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 


- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r}
heart_bag<-randomForest(
  as.factor(AHD)~.,
  data = heart_train,
  mtry = ncol(heart_train) - 1,
  na.action = na.omit
)
# oob error rate
sum(heart_bag$err.rate[, 1])

varImpPlot(
  heart_bag,
  cex.lab = 1.5, cex.axis = 2, cex = 1.3,
  n.var = 13, col = "red4", main = "", pch = 16)

importance(heart_bag)
```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r}
heart_rf <- randomForest(
  as.factor(AHD)~.,
  data = heart_train,
  na.action = na.omit
)
#oob error rate
sum(heart_rf$err.rate[, 1])

varImpPlot(
  heart_rf,
  cex.lab = 1.5, cex.axis = 2, cex = 1.3,
  n.var = 13, col = "red4", main = "", pch = 16)

importance(heart_rf)
```

---

# Question 3: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r}
heart_boost = gbm(AHD ~ ., 
                  data = heart_train, 
                  distribution = "bernoulli",
                  n.trees = 3000,
                  cv.folds = 5,
                  class.stratify.cv = TRUE
                  )

# make chart pretties y-axis
y_lim_max <- max(max(heart_boost$train.error), max(heart_boost$cv.error)) + 0.05
plot(heart_boost$train.error, xlab = "Boosting Iterations", ylab = "5-Fold CV Error", 
     ylim = c(0, y_lim_max), col = "red")
lines(heart_boost$cv.error, col = 'blue')
legend("right", legend = c("Training Error", "5-Fold CV Error"), 
       col = c("red", "blue"), lty = 1, cex = 0.8, inset = c(0.1, 0))

summary(heart_boost)
```

---


## Question 4: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.


- Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r}
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
plot(heart_boost$train.error, xlab = "Iterations", ylab = "10-Fold CV Error", col = "red")
lines(heart_boost$cv.error, col = 'blue')
legend("topright", legend = c("Training Error", "10-Fold CV Error"), 
       col = c("red", "blue"), lty = 1, cex = 0.8)

#calculate MSE
# predict with test data and mse
yhat_boost <- predict(heart_boost, 
                      newdata = heart_test, 
                      n.trees = 5000, 
                      type = "response")

mse_test <- mean((yhat_boost - heart_test$AHD)^2)
print(mse_test)

```

- Repeat the above using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: 
a) ``shrinkage = 0.001``, ``interaction.depth = 2``; 
b) ``shrinkage = 0.01``, ``interaction.depth = 1``; 
c) ``shrinkage = 0.01``, ``interaction.depth = 2``.

```{r}
set.seed(301)
# build 3 more trees and repeat above

# (a)
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 2, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
plot(heart_boost$train.error, xlab = "Iterations", ylab = "10-Fold CV Error", col = "red")
lines(heart_boost$cv.error, col = 'blue')
legend("topright", legend = c("Training Error", "10-Fold CV Error"), 
       col = c("red", "blue"), lty = 1, cex = 0.8)

yhat_boost <- predict(heart_boost, 
                      newdata = heart_test, 
                      n.trees = 5000, 
                      type = "response")
mse_test <- mean((yhat_boost - heart_test$AHD)^2)
print(mse_test)

```

```{r}
# (b)
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.01, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
plot(heart_boost$train.error, xlab = "Iterations", ylab = "10-Fold CV Error", col = "red")
lines(heart_boost$cv.error, col = 'blue')
legend("topright", legend = c("Training Error", "10-Fold CV Error"), 
       col = c("red", "blue"), lty = 1, cex = 0.8)

yhat_boost <- predict(heart_boost, 
                      newdata = heart_test, 
                      n.trees = 5000, 
                      type = "response")
mse_test <- mean((yhat_boost - heart_test$AHD)^2)
print(mse_test)
```

```{r}
# (c)
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart[train, ], distribution = "bernoulli", n.trees = 5000, interaction.depth = 2, shrinkage = 0.01, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
plot(heart_boost$train.error, xlab = "Iterations", ylab = "10-Fold CV Error", col = "red")
lines(heart_boost$cv.error, col = 'blue')
legend("topright", legend = c("Training Error", "10-Fold CV Error"), 
       col = c("red", "blue"), lty = 1, cex = 0.8)

yhat_boost <- predict(heart_boost, 
                      newdata = heart_test, 
                      n.trees = 5000, 
                      type = "response")
mse_test <- mean((yhat_boost - heart_test$AHD)^2)
print(mse_test)
```

## Question 5: Extreme Gradient Boosting

Training an xgboost model with `xgboost` and perform a grid search for tuning the number of trees and the maxium depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

```{r warning=FALSE}

train_control = caret::trainControl(method = "cv", number = 10, search ="grid")

tune_grid<-  expand.grid(max_depth = c(1, 3, 5, 7), #larger makes model more complex and potentially overfit
                        nrounds = (1:10)*50, # number of trees
                        # default values below
                        eta = c(0.01,0.1,0.3), # learning rate (equivalent to shrinkage)
                        gamma = 0, #Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6 #specify the fraction of columns to be subsampled
                        )

heart_xgb<-caret::train(
  AHD ~ .,
  data = heart_train,
  na.action = na.omit,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = tune_grid,
  verbosity = 0
)

varimp <- varImp(heart_xgb)
plot(varimp)

# predict using the test data compute mse
predictions <- predict(heart_xgb, newdata = heart_test)
mse_test <- mean((predictions - heart_test$AHD)^2)
print(mse_test)
```


- Compare the the performance of the different models and summarize

In our case, the gradient boosting method with the initial hyperparameters give the best results in terms of MSE: 0.1395. Extreme Gradient Boosting gives 0.1518, but I think we can probably improve it with more tuning. These tree-based methods still serve as important cornerstone for modelling structured tabular data as they provide stronger inductive biases than neural networks on these types of data. As we go from the simple trees to gradient boosting, the process for training models get more and more complicated with the addition of many hyperparameters, and we gradually lose the ability to easily interpret the model. The choice of the model should depend on the specific use cases and applications.

# Deliverables

1. Questions 1-5 answered, pdf or html output uploaded to quercus
